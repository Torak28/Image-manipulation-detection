\chapter{Analiza istniejących metod w obrębie dziedziny}

Badanie manipulacji zdjęć jest dość obszernym dziedziną w której to, co więcej, można wyróżnić szereg podejść i prób zrozumienia problemu. Stanem wiedzy o podejściach statystycznych, lub takich opartych o metadane zdjęcia jest książka \textit{Photo Forensics}, autorstwa Hany Farida \cite{forensics}. Podejścia oparte o analizę poszczególnych zdjęć bez używania uczenia maszynowego w większości opierają się o metaznaczniki zawarte w strukturze plików *.JPEG. Metaznaczniki te są zarówno wspierane po stronie aparatów cyfrowych jak i oprogramowania do edycji zdjęć. Dla zdjęć cyfrowych generowane są dane dotyczące modelu aparatu, jego ustawień(takich jak czas naświetlania, wartość przesłony czy czułość matrycy w ISO), daty wykonania zdjęcia, czy nawet współrzędne GPS miejsca w którym zdjęcie zostało zrobione. Dla programów do obróbki zdjęć generowane są informację o nazwie użytego programu i w zależności od rodzaju oprogramowania - część danych utworzonych przez aparat zostaje \textit{wymazana}. Takie podejście sprawia, że do sprawdzenia czy dane zdjęcie zostało przerobione, wystarczy przeczytać jego metadane \cite{forensics}.\\

Innym podejściem, choć ciągle bazującym na metaznacznikach jest zbudowanie \textit{odcisku palca} aparatu o zadanych ustawieniach i porównywaniu spreparowanych zdjęć do obrazów rozpatrywanych jako potencjalnie przerobione \cite{fingerprints}. Problemów z zaproponowanym przez A. Swaminathan, M. Wu oraz K. J. Ray Liu jest kilka i sami wspominają o nich w swojej pracy \cite{fingerprints}: całość wymaga zbudowania modelu poszczególnych aparatów, co znacząco zimniejsza możliwości skalowania aplikacji oraz dodatkowo, im więcej rozpatrywanych aparatów, tym mniejsza dokładność predykcji - aparaty tego samego producenta, będące nie daleko od siebie pod względem rodziny modelu, produkują bardzo podobne \textit{odciski palców}. Co więcej, do działania i samego treningu, opisywany model wymaga zdjęć z wypełnionymi znacznikami.\\

Inną rodziną podejść są te zorientowane na pomijanie metadanych, podział zdjęcia na mniejsze części, a następnie sprawdzanie ich właściwości względem siebie. Przykładem może być praca z 2015 roku autorstwa  M. Goljan and J. Fridrich, w której opisują stworzony przez siebie wariant metody CFA(z ang. \textit{color filter array}), nazwany przez siebie CRM(z ang. \textit{color rich model}) \cite{goljan}. Jego zadaniem jest stworzenie szeregu statystyk opisujących relacje pomiędzy kolorami pikseli w określonym obszarze. Następnie mając te informacje sprawdzane jest czy występują w obrazie nagłe przejścia koloru w których zbierane statystyki wskazują na obiekt spoza oryginalnej przestrzeni. Takie podejście świetnie sprawdza się w sytuacji w której manipulacja polega na wstawieniu obcego elementu w zdjęcie. Problemem są oczywiście takie manipulacje, które ingerują w cały obszar zdjęcia, a mówiąc bardziej szczegółowo, w jego przestrzeń kolorów.\\

Podobny do opisanego powyżej jest pomysł pracy autorstwa D. Cozzolino, G. Poggi, L. Verdoliva z 2015 roku, z tą różnicą że zamiast przestrzeni barw autorzy sprawdzają szum występujący pomiędzy pikselami poszczególnych elementów zdjęcia \cite{poggi}. Nagłe zmiany, lub pewne nieciągłości pozwalają autorom nie tylko stwierdzić czy zdjęcie zostało przerobione czy nie - pozwala też zlokalizować miejsce w którym do takiej manipulacji potencjalnie doszło.\\

Innym podejściem bazującym na poprzednich ale jednak bardziej zwróconym w stronę uczenia głębokiego, była praca naukowców z USA z 2017 roku \cite{lstm}. Podobnie jak poprzednicy, wykorzystali podział obrazu na mniejsze elementy z tą jednak różnicą, że tym razem nie definiowali funkcji ekstrahującej z pomniejszych elementów cech - zamiast tego stanowiły one wejście klasyfikatora LSTM, którego pamięć została wykorzystana do przetworzenia pojedynczego zdjęcia. Z racji jednak że badali oni zmiany lokale natrafili na ograniczenia takie same jak inni uczeni \cite{goljan} \cite{poggi}.\\

Kolejnym typem prób rozpoznawania czy dane zdjęcie zostało zmanipulowane czy nie jest wykorzystanie podwójnej kompresji JPEG - skorzystanie z algorytmu ELA(z ang. \textit{Error Level Analysis}), co zostało opisane dość dokładnie w pracy N. Krawetza z 2007 roku\cite{hacker}. Podczas zapisywania plików JPEG dochodzi do kompresji danych, domyślnie wartość kompresji jest ustawiona na $90\%$. Oznacza to że algorytm będzie przeglądał obszary, o wielkości 8x8 pikseli, w celu zaoszczędzenia $\sim10\%$ danych. Ideą algorytmu ELA jest zapisanie zdjęcia podwójnie z kompresją pomiędzy $\sim80\%$ - $\sim90\%$, a następnie \textit{odjęcie} od zdjęcia oryginalnego. Taki zabieg sprawi że dojdzie do wytworzeniu artefaktów kompresji - miejsc gdzie \textit{poziom} uproszczenia jest różny dla elementów na zdjęciu. Tym samym będzie to oznaczało, że elementy te przed zastosowaniem algorytmu ELA były różnej jakości, co wskazuje że nie należą do tego samego obrazka \cite{oczko}.\\

Jednym z ciekawszych i najbliższych mojej propozycji rozwiązania problemu jest praca z 2016 roku, która wykorzystuje autoencoder \cite{auto}. Ideą pracy autoencodera jest kompresja informacji wejściowej, a następnie próba odtworzenia jej w jak najlepszym stopniu. Okazuje się, że gdy zbadamy obraz oryginalny, skompresowany i odtworzony to wystąpią różnice w jakości odtwarzania w zależności od tego czy dany obszar był poddany manipulacji czy nie. Tym samym porównując te trzy zdjęcia możemy nie tylko stwierdzić czy doszło do manipulacji, ale też wskazać obszar w którym coś zostało zmienione.
