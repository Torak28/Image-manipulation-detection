{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprawdzanie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie odpowiednich danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeśli zajdzie potrzeba\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dla PoC wykonuje obliczenia dla:\n",
    " * '../data/DogsCats'\n",
    "Folder docelowy:\n",
    " * '../data/Photos'\n",
    "'''\n",
    "\n",
    "dir_path = '../data/Photos'\n",
    "A_folder = 'originals'\n",
    "B_folder = 'photoshops'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Załadowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "# fix random bo tak ( ͡° ͜ʖ ͡°)\n",
    "odp = 42\n",
    "numpy.random.seed(odp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stałe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilkości odpowiednie dla VGG\n",
    "\n",
    "IMAGE_WIDTH=224\n",
    "IMAGE_HEIGHT=224\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie Danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Opis danych:\n",
    "1 - klasa 1 -> Originals\n",
    "0 - klasa 2 -> Photoshops\n",
    "''' \n",
    "\n",
    "A_folder_list = os.listdir(dir_path + '/' + A_folder)\n",
    "B_folder_list = os.listdir(dir_path + '/' + B_folder)\n",
    "\n",
    "filenames = []\n",
    "categories = []\n",
    "\n",
    "for filename in A_folder_list:\n",
    "    categories.append(1)\n",
    "    filenames.append(dir_path + '/' + A_folder + '/' + filename)\n",
    "\n",
    "for filename in B_folder_list:\n",
    "    categories.append(0)\n",
    "    filenames.append(dir_path + '/' + B_folder + '/' + filename)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mieszamy!\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(df['filename'])\n",
    "image = load_img(sample)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obróbka danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''\n",
    "Podział danych z całego df na X i y:\n",
    "\n",
    "X - wszystko oprócz category\n",
    "y - category\n",
    "'''\n",
    "\n",
    "X, y = df.iloc[:, 0:-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% danych do testów\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = odp)\n",
    "result = next(kf.split(X, y))\n",
    "\n",
    "'''\n",
    "Podział danych z całego df na podstawie splitu\n",
    "\n",
    "train - 80% danych\n",
    "test  - 20% danych\n",
    "'''\n",
    "train = df.iloc[result[0]]\n",
    "test =  df.iloc[result[1]]\n",
    "\n",
    "# Reset indeksów\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Kształt danych:')\n",
    "print(f'\\t df: {df.shape}')\n",
    "\n",
    "print(f'\\t train: {train.shape}')\n",
    "print(f'\\t test: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upewnienie się że dane w train[filename] i test[filename] są stringiem - lepsze dla generatorów\n",
    "\n",
    "train['filename'] = train['filename'].astype(str) \n",
    "test['filename'] = test['filename'].astype(str)\n",
    "\n",
    "# Upewnienie się że dane w train[category] i test[category] są stringiem - lepsze dla generatorów\n",
    "\n",
    "train['category'] = train['category'].astype(str) \n",
    "test['category'] = test['category'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Kształt danych:')\n",
    "print(f'\\t df: {df.shape}')\n",
    "\n",
    "print(f'\\t train: {train.shape}')\n",
    "print(f'\\t test: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcję liczące statystyki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countStats(_y_true, _y_pred):\n",
    "    accuracy = accuracy_score(_y_true, _y_pred, normalize=True)\n",
    "    precision = precision_score(_y_true, _y_pred, average='weighted')\n",
    "    recall = recall_score(_y_true, _y_pred, average='weighted')\n",
    "    fscore = f1_score(_y_true, _y_pred, average='weighted')\n",
    "    \n",
    "    return accuracy, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Źrodło:\n",
    "https://medium.com/@aakashgoel12/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n",
    "'''\n",
    "\n",
    "def get_f1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "def plot_cm(cm, classes):\n",
    "    plot_confusion_matrix(conf_mat=cm,\n",
    "                          colorbar=True, \n",
    "                          show_absolute=True,\n",
    "                          show_normed=True,\n",
    "                          class_names=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cb_early_stopper - skończenie uczenia kiedy val_loss nie będzie się poprawiać przez 10 epok\n",
    "cb_checkpointer - zapis modelu do pliku 'best.h5' modeli o najlepszym(najmniejszym) val_loss\n",
    "cb_learning_rate_reduction - zmniejszenie LR jeśli val_loss nie będzie się poprawiać przez 5 epok\n",
    "'''\n",
    "\n",
    "EARLY_STOP_PATIENCE = 15\n",
    "LEARNING_RATE_PATIENCE = 5\n",
    "\n",
    "cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE, verbose=1)\n",
    "cb_checkpointer = ModelCheckpoint(filepath = 'best.h5', monitor = 'val_loss', save_best_only = True, verbose=1)\n",
    "cb_learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=LEARNING_RATE_PATIENCE, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dobór parametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do\n",
    "\n",
    "batch_size = 16\n",
    "activation = 'relu'\n",
    "loss_type = 'binary_crossentropy'\n",
    "optimizer = 'Adam'\n",
    "dropout = 0.25\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generator dla danych trenningowych\n",
    "'''\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "data_gen = ImageDataGenerator(\n",
    "#     preprocessing_function=preprocess_input\n",
    "    rescale=1./255,\n",
    "#     zoom_range=[0, 0.2],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "data_test_gen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "train_generator = data_gen.flow_from_dataframe(\n",
    "    dataframe = train,\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    class_mode='binary',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generator dla danych testowych\n",
    " * reskalowanie\n",
    "'''\n",
    "\n",
    "test_generator = data_test_gen.flow_from_dataframe( \n",
    "    dataframe = test,\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    class_mode='binary',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = train.sample(n=1).reset_index(drop=True)\n",
    "\n",
    "example_generator = data_gen.flow_from_dataframe(\n",
    "    dataframe = example_df,\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    class_mode='categorical',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(0, 15):\n",
    "    plt.subplot(5, 3, i+1)\n",
    "    for X_batch, Y_batch in example_generator:\n",
    "        image = X_batch[0]\n",
    "        plt.imshow(image)\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.applications import VGG16\n",
    "\n",
    "pre_trained_model_fn = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "\n",
    "for layer in pre_trained_model_fn.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in pre_trained_model_fn.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_fn = Sequential()\n",
    "\n",
    "model_fn.add(pre_trained_model_fn)\n",
    "model_fn.add(Flatten())\n",
    "model_fn.add(Dropout(dropout))\n",
    "model_fn.add(Dense(256, activation=activation))\n",
    "model_fn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_fn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn.compile(loss=loss_type, optimizer=optimizer, metrics=['accuracy', get_f1])\n",
    "\n",
    "history_fn = model_fn.fit_generator(\n",
    "    train_generator, \n",
    "    epochs = epochs,\n",
    "    validation_data = test_generator,\n",
    "    validation_steps = test.shape[0]//batch_size,\n",
    "    steps_per_epoch = train.shape[0]//batch_size,\n",
    "    callbacks = [cb_checkpointer, cb_early_stopper, cb_learning_rate_reduction]\n",
    ")\n",
    "\n",
    "# Wczytanie najlepszego\n",
    "model_fn.load_weights('best.h5')\n",
    "\n",
    "# Zapis\n",
    "model_fn.save('the_best_fn_P.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statystyki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "# Wykres loss\n",
    "ax1.plot(history_fn.history['loss'], color='b', label=\"Training loss\")\n",
    "ax1.plot(history_fn.history['val_loss'], color='r', label=\"Validation loss\")\n",
    "ax1.set_xticks(numpy.arange(0, len(history_fn.history['val_loss']), 1))\n",
    "ax1.legend(loc='best', shadow=True)\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "\n",
    "# Wykres accuracy\n",
    "ax2.plot(history_fn.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax2.plot(history_fn.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "ax2.set_xticks(numpy.arange(0, len(history_fn.history['val_accuracy']), 1))\n",
    "ax2.legend(loc='best', shadow=True)\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.set_xlabel('epoch')\n",
    "\n",
    "# Wykres F1\n",
    "ax3.plot(history_fn.history['get_f1'], color='b', label=\"Training f1\")\n",
    "ax3.plot(history_fn.history['val_get_f1'], color='r',label=\"Validation f1\")\n",
    "ax3.set_xticks(numpy.arange(0, len(history_fn.history['get_f1']), 1))\n",
    "ax3.legend(loc='best', shadow=True)\n",
    "ax3.set_ylabel('f1')\n",
    "ax3.set_xlabel('epoch')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame({\n",
    "    'filename': test['filename'],\n",
    "    'category': test['category'],\n",
    "    'predict_fn': None\n",
    "})\n",
    "\n",
    "# Mieszamy!\n",
    "stats_df = stats_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_generator = data_gen.flow_from_dataframe( \n",
    "    dataframe = stats_df,\n",
    "    x_col = 'filename',\n",
    "    class_mode=None,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = model_fn.predict_generator(validate_generator, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zamiana z one hot encodera na jedno wyjście\n",
    "stats_df['predict_fn'] = numpy.argmax(predict_fn, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df['predict_fn'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_fn = stats_df['category'].to_numpy().astype(int)\n",
    "y_pred_fn = stats_df['predict_fn'].to_numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_fn = confusion_matrix(y_true_fn, y_pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(cm_fn, ['Originals', 'Photoshops'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, get_f1  = model_fn.evaluate_generator(test_generator, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test loss: {loss:.3}')\n",
    "print(f'Test accuracy: {accuracy:.3}')\n",
    "print(f'Test F1: {get_f1:.3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
