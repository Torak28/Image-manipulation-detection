{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprawdzanie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie odpowiednich danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dla PoC wykonuje obliczenia dla:\n",
    " * '../data/DogsCats'\n",
    "Folder docelowy:\n",
    " * '../data/Photos'\n",
    "'''\n",
    "\n",
    "dir_path = '../data/Photos'\n",
    "A_folder = 'originals'\n",
    "B_folder = 'photoshops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "# fix random bo tak ( ͡° ͜ʖ ͡°)\n",
    "odp = 42\n",
    "numpy.random.seed(odp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stałe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilkości\n",
    "\n",
    "IMAGE_WIDTH=128\n",
    "IMAGE_HEIGHT=128\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie Danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Opis danych:\n",
    "1 - klasa 1 -> Originals\n",
    "0 - klasa 2 -> Photoshops\n",
    "''' \n",
    "\n",
    "A_folder_list = os.listdir(dir_path + '/' + A_folder)\n",
    "B_folder_list = os.listdir(dir_path + '/' + B_folder)\n",
    "\n",
    "filenames = []\n",
    "categories = []\n",
    "\n",
    "for filename in A_folder_list:\n",
    "    categories.append(0)\n",
    "    filenames.append(dir_path + '/' + A_folder + '/' + filename)\n",
    "\n",
    "for filename in B_folder_list:\n",
    "    categories.append(1)\n",
    "    filenames.append(dir_path + '/' + B_folder + '/' + filename)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mieszamy!\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(df['filename'])\n",
    "image = load_img(sample)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obróbka zdjęć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_image = numpy.array(image)\n",
    "plt.imshow(np_image)\n",
    "plt.title('Obraz źródłowy')\n",
    "plt.show()\n",
    "\n",
    "np_gray = rgb2gray(np_image)\n",
    "plt.imshow(np_gray, cmap='gray')\n",
    "plt.title('Obraz czarno-biały')\n",
    "plt.show()\n",
    "\n",
    "hog_features, hog_image = hog(np_gray,\n",
    "                              visualize=True,\n",
    "                              block_norm='L2-Hys',\n",
    "                              pixels_per_cell=(32, 32))\n",
    "\n",
    "plt.imshow(hog_image, cmap='gray')\n",
    "plt.title('Histogram kierunkowych gradientów')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mahotas\n",
    "import cv2\n",
    "\n",
    "def ft_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hu_moments = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return hu_moments\n",
    "\n",
    "def ft_haralick(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mahotas.features.haralick(image).mean(axis=0)\n",
    "    return haralick\n",
    "\n",
    "def ft_histogram(image, mask=None):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # chanels: [0,1,2], bo mamy przestrzeń HSV\n",
    "    # mask: None\n",
    "    # histSize: [8, 8, 8], bin count, po 8 dla każdego z kanałów\n",
    "    # ranges : [0, 256, 0, 256, 0, 256], wszystko dla każdego z 3 kanałów\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def ft_hog(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hog_features = hog(image, block_norm='L2-Hys', pixels_per_cell=(32, 32))\n",
    "    return hog_features\n",
    "\n",
    "def preprocess_image(image_path, verbose=False):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, IMAGE_SIZE)\n",
    "    histogram = ft_histogram(image)\n",
    "    haralick = ft_haralick(image)\n",
    "    hu = ft_hu_moments(image)\n",
    "    hog = ft_hog(image)\n",
    "    global_feature = numpy.hstack([histogram, haralick, hu, hog])\n",
    "    if verbose:\n",
    "        print(f'Histagram shape: {histogram.shape}')\n",
    "        print(f'harlick shape: {haralick.shape}')\n",
    "        print(f'Hu shape: {hu.shape}')\n",
    "        print(f'Hog shape{hog.shape}')\n",
    "        print('')\n",
    "    return global_feature\n",
    "\n",
    "def preprocess_image_no_hog(image_path, verbose=False):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, IMAGE_SIZE)\n",
    "    histogram = ft_histogram(image)\n",
    "    haralick = ft_haralick(image)\n",
    "    hu = ft_hu_moments(image)\n",
    "    global_feature = numpy.hstack([histogram, haralick, hu])\n",
    "    if verbose:\n",
    "        print(f'Histagram shape: {histogram.shape}')\n",
    "        print(f'harlick shape: {haralick.shape}')\n",
    "        print(f'Hu shape: {hu.shape}')\n",
    "        print('')\n",
    "    return global_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przeliczenie Cech Zdjęć + Kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_f = []\n",
    "g_f_n_h = []\n",
    "l = []\n",
    "\n",
    "for i in df['filename']:\n",
    "    data_img = preprocess_image(i)\n",
    "    data_img_no_hog = preprocess_image_no_hog(i)\n",
    "    g_f.append(data_img)\n",
    "    g_f_n_h.append(data_img_no_hog)\n",
    "\n",
    "for i in df['category']:\n",
    "    if i == 0:\n",
    "        l.append('original')\n",
    "    else:\n",
    "        l.append('photoshop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(f'Wektor zdjęć z HOG: {numpy.array(g_f).shape}')\n",
    "print(f'Wektor zdjęć bez HOG: {numpy.array(g_f_n_h).shape}')\n",
    "print(f'Wektor kategorii słownych: {numpy.array(l).shape}\\n')\n",
    "\n",
    "# Label Encoder\n",
    "# Koty - 0\n",
    "# Psy - 1\n",
    "targetNames = numpy.unique(l)\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(l)\n",
    "\n",
    "# Scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaled_features_gf = scaler.fit_transform(g_f)\n",
    "rescaled_features_gfnh = scaler.fit_transform(g_f_n_h)\n",
    "\n",
    "print(f'Wektor kategorii liczbowych: {target.shape}')\n",
    "print(f'Skalowany wektor zdjęć z HOG: {numpy.array(rescaled_features_gf).shape}')\n",
    "print(f'Skalowany wektor zdjęć bez HOG: {numpy.array(rescaled_features_gfnh).shape}\\n')\n",
    "\n",
    "print(f'Max arg z nieskalowanego(z HOG): {numpy.argmax(g_f[0])}, Min arg z nieskalowanego(z HOG): {numpy.argmin(g_f[0])}')\n",
    "print(f'Max z nieskalowanego(z HOG): {numpy.amax(g_f[0])}, Min z nieskalowanego(z HOG): {numpy.amin(g_f[0])}\\n')\n",
    "\n",
    "print(f'Max arg z nieskalowanego(bez HOG): {numpy.argmax(g_f_n_h[0])}, Min arg z nieskalowanego(bez HOG): {numpy.argmin(g_f_n_h[0])}')\n",
    "print(f'Max z nieskalowanego(bez HOG): {numpy.amax(g_f_n_h[0])}, Min z nieskalowanego(bez HOG): {numpy.amin(g_f_n_h[0])}\\n')\n",
    "\n",
    "print(f'Max arg z skalowanego(z HOG): {numpy.argmax(rescaled_features_gf[0])}, Min arg z skalowanego(z HOG): {numpy.argmin(rescaled_features_gf[0])}')\n",
    "print(f'Max z skalowanego(z HOG): {numpy.amax(rescaled_features_gf[0])}, Min z skalowanego(z HOG): {numpy.amin(rescaled_features_gf[0])}\\n')\n",
    "\n",
    "print(f'Max arg z skalowanego(bez HOG): {numpy.argmax(rescaled_features_gfnh[0])}, Min arg z skalowanego(bez HOG): {numpy.argmin(rescaled_features_gfnh[0])}')\n",
    "print(f'Max z skalowanego(bez HOG): {numpy.amax(rescaled_features_gfnh[0])}, Min z skalowanego(bez HOG): {numpy.amin(rescaled_features_gfnh[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zapis/Odczyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save(features, labels, dataframe, name):\n",
    "    h5f_data = h5py.File('data_' + str(name) + '.h5', 'w')\n",
    "    h5f_data.create_dataset('dataset', data=numpy.array(features))\n",
    "\n",
    "    h5f_label = h5py.File('labels_' + str(name) + '.h5', 'w')\n",
    "    h5f_label.create_dataset('dataset', data=numpy.array(labels))\n",
    "\n",
    "    h5f_data.close()\n",
    "    h5f_label.close()\n",
    "\n",
    "    dataframe.to_csv('dataframe_' + str(name) + '.csv')\n",
    "    \n",
    "def load(features, labels, dataframe):\n",
    "    h5f_data  = h5py.File(features, 'r')\n",
    "    h5f_label = h5py.File(labels, 'r')\n",
    "\n",
    "    global_features_string = h5f_data['dataset']\n",
    "    global_labels_string   = h5f_label['dataset']\n",
    "\n",
    "    global_features = numpy.array(global_features_string)\n",
    "    global_labels   = numpy.array(global_labels_string)\n",
    "\n",
    "    h5f_data.close()\n",
    "    h5f_label.close()\n",
    "    \n",
    "    df = pd.read_csv(dataframe, index_col = 0)  \n",
    "    \n",
    "    return global_features, global_labels, df\n",
    "    \n",
    "save(rescaled_features_gf, target, df, name='Photos_with_HOG')\n",
    "save(rescaled_features_gfnh, target, df, name='Photos_without_HOG')\n",
    "\n",
    "# B od BIG - więcej danych, S - small\n",
    "global_features_B, global_labels_B, df_B = load('data_Photos_with_HOG.h5', 'labels_Photos_with_HOG.h5', 'dataframe_Photos_with_HOG.csv')\n",
    "global_features_S, global_labels_S, df_S = load('data_Photos_without_HOG.h5', 'labels_Photos_without_HOG.h5', 'dataframe_Photos_without_HOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Z HOG:')\n",
    "print(f'\\t Wektor zdjęć: {global_features_B.shape}')\n",
    "print(f'\\t Wektor kategorii słownych: {global_labels_B.shape}\\n')\n",
    "print(f'\\t Wektor dataframe: {df_B.shape}\\n\\n')\n",
    "\n",
    "print(f'BEZ HOG:')\n",
    "print(f'\\t Wektor zdjęć: {global_features_S.shape}')\n",
    "print(f'\\t Wektor kategorii słownych: {global_labels_S.shape}\\n')\n",
    "print(f'\\t Wektor dataframe: {df_S.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_S.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcję liczące statystyki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "def countStats(_y_true, _y_pred):\n",
    "    accuracy = accuracy_score(_y_true, _y_pred, normalize=True)\n",
    "    precision = precision_score(_y_true, _y_pred, average='weighted')\n",
    "    recall = recall_score(_y_true, _y_pred, average='weighted')\n",
    "    fscore = f1_score(_y_true, _y_pred, average='weighted')\n",
    "    \n",
    "    return accuracy, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "def plot_cm(cm, classes):\n",
    "    plot_confusion_matrix(conf_mat=cm,\n",
    "                          colorbar=True, \n",
    "                          show_absolute=True,\n",
    "                          show_normed=True,\n",
    "                          class_names=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcje do liczenia modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def do_the_thing(features, labels, pca_val=False):\n",
    "    \n",
    "    tcm_list = []\n",
    "    tAccuracy_list = []\n",
    "    tPrecision_list = []\n",
    "    tRecall_list = []\n",
    "    tFScore_list = []\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=odp)\n",
    "    \n",
    "    if pca_val:\n",
    "        pca = PCA(n_components=532)\n",
    "        features = pca.fit_transform(features)\n",
    "    \n",
    "    for train_index, test_index in kf.split(global_features_S, global_labels_S):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "        svm = SVC(kernel='linear', probability=True, random_state=odp, verbose=True)\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = svm.predict(X_test)\n",
    "        accuracy, precision, recall, fscore = countStats(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        tAccuracy_list.append(accuracy)\n",
    "        tPrecision_list.append(precision)\n",
    "        tRecall_list.append(recall)\n",
    "        tFScore_list.append(fscore)\n",
    "        tcm_list.append(cm)\n",
    "\n",
    "    \n",
    "    print(f'Kształt danych:')\n",
    "    print(f'\\t X_train: {X_train.shape}')\n",
    "    print(f'\\t X_test: {X_test.shape}')\n",
    "    print(f'\\t y_train: {y_train.shape}')\n",
    "    print(f'\\t y_test: {y_test.shape}')\n",
    "    \n",
    "    accuracy = numpy.mean(tAccuracy_list)\n",
    "    precision = numpy.mean(tPrecision_list)\n",
    "    recall = numpy.mean(tRecall_list)\n",
    "    fscore = numpy.mean(tFScore_list)\n",
    "    \n",
    "    print(f'\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF-score: {fscore}\\n')\n",
    "    \n",
    "    cm = sum(tcm_list)\n",
    "    \n",
    "    plot_cm(cm, ['Originals', 'Photoshops'])\n",
    "    \n",
    "    print(f'\\n{cm}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bez HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_the_thing(global_features_S, global_labels_S, pca_val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z HOG -> PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_the_thing(global_features_B, global_labels_B, pca_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dobór parametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TBD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "param_grid = [\n",
    "                {'kernel': ['rbf'], 'gamma': ['scale', 'auto'],'C': [1, 10, 100, 1000]},\n",
    "                {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}                             \n",
    "              ]\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=2, verbose=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Najlepsze acc: %f używając %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# Wyniki!\n",
    "grid_result.best_params_\n",
    "'''\n",
    "\n",
    "print('TBD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ToDo:\n",
    " - [ ] GridSearch\n",
    " - [ ] Sprawdzić output - predict\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
