{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprawdzanie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie odpowiednich danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dla PoC wykonuje obliczenia dla:\n",
    " * '../data/DogsCats'\n",
    "Folder docelowy:\n",
    " * '../data/Photos'\n",
    "'''\n",
    "\n",
    "dir_path = '../../data/Photos'\n",
    "A_folder = 'originals'\n",
    "B_folder = 'photoshops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "# fix random bo tak ( ͡° ͜ʖ ͡°)\n",
    "odp = 42\n",
    "numpy.random.seed(odp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stałe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilkości\n",
    "\n",
    "IMAGE_WIDTH=128\n",
    "IMAGE_HEIGHT=128\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie Danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Opis danych:\n",
    "1 - klasa 1 -> Originals\n",
    "0 - klasa 2 -> Photoshops\n",
    "''' \n",
    "\n",
    "A_folder_list = os.listdir(dir_path + '/' + A_folder)\n",
    "B_folder_list = os.listdir(dir_path + '/' + B_folder)\n",
    "\n",
    "filenames = []\n",
    "categories = []\n",
    "\n",
    "for filename in A_folder_list:\n",
    "    categories.append(0)\n",
    "    filenames.append(dir_path + '/' + A_folder + '/' + filename)\n",
    "\n",
    "for filename in B_folder_list:\n",
    "    categories.append(1)\n",
    "    filenames.append(dir_path + '/' + B_folder + '/' + filename)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mieszamy!\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(df['filename'])\n",
    "image = load_img(sample)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obróbka zdjęć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mahotas\n",
    "import cv2\n",
    "\n",
    "def ft_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hu_moments = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return hu_moments\n",
    "\n",
    "def ft_haralick(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mahotas.features.haralick(image).mean(axis=0)\n",
    "    return haralick\n",
    "\n",
    "def ft_histogram(image, mask=None):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # chanels: [0,1,2], bo mamy przestrzeń HSV\n",
    "    # mask: None\n",
    "    # histSize: [8, 8, 8], bin count, po 8 dla każdego z kanałów\n",
    "    # ranges : [0, 256, 0, 256, 0, 256], wszystko dla każdego z 3 kanałów\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def ft_hog(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hog_features = hog(image, block_norm='L2-Hys', pixels_per_cell=(32, 32))\n",
    "    return hog_features\n",
    "\n",
    "def preprocess_image(image_path, hog=True):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, IMAGE_SIZE)\n",
    "        histogram = ft_histogram(image)\n",
    "        haralick = ft_haralick(image)\n",
    "        hu = ft_hu_moments(image)\n",
    "        if hog == True:\n",
    "            hog = ft_hog(image)\n",
    "            global_feature = numpy.hstack([histogram, haralick, hu, hog])\n",
    "        elif hog == False:\n",
    "            global_feature = numpy.hstack([histogram, haralick, hu])\n",
    "        return global_feature\n",
    "    except Exception as e:\n",
    "        print(f'Problem with {image_path}, error_msg: {str(e)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przeliczenie Cech Zdjęć + Kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_f = []\n",
    "g_f_n_h = []\n",
    "l = []\n",
    "\n",
    "for i in df['filename']:\n",
    "    data_img = preprocess_image(i)\n",
    "    data_img_no_hog = preprocess_image(i, hog=False)\n",
    "    g_f.append(data_img)\n",
    "    g_f_n_h.append(data_img_no_hog)\n",
    "\n",
    "for i in df['category']:\n",
    "    if i == 0:\n",
    "        l.append('original')\n",
    "    else:\n",
    "        l.append('photoshop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(f'Wektor zdjęć z HOG: {numpy.array(g_f).shape}')\n",
    "print(f'Wektor zdjęć bez HOG: {numpy.array(g_f_n_h).shape}')\n",
    "print(f'Wektor kategorii słownych: {numpy.array(l).shape}\\n')\n",
    "\n",
    "# Label Encoder\n",
    "# Koty - 0\n",
    "# Psy - 1\n",
    "targetNames = numpy.unique(l)\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(l)\n",
    "\n",
    "# Scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaled_features_gf = scaler.fit_transform(g_f)\n",
    "rescaled_features_gfnh = scaler.fit_transform(g_f_n_h)\n",
    "\n",
    "print(f'Wektor kategorii liczbowych: {target.shape}')\n",
    "print(f'Skalowany wektor zdjęć z HOG: {numpy.array(rescaled_features_gf).shape}')\n",
    "print(f'Skalowany wektor zdjęć bez HOG: {numpy.array(rescaled_features_gfnh).shape}\\n')\n",
    "\n",
    "print(f'Max arg z nieskalowanego(z HOG): {numpy.argmax(g_f[0])}, Min arg z nieskalowanego(z HOG): {numpy.argmin(g_f[0])}')\n",
    "print(f'Max z nieskalowanego(z HOG): {numpy.amax(g_f[0])}, Min z nieskalowanego(z HOG): {numpy.amin(g_f[0])}\\n')\n",
    "\n",
    "print(f'Max arg z nieskalowanego(bez HOG): {numpy.argmax(g_f_n_h[0])}, Min arg z nieskalowanego(bez HOG): {numpy.argmin(g_f_n_h[0])}')\n",
    "print(f'Max z nieskalowanego(bez HOG): {numpy.amax(g_f_n_h[0])}, Min z nieskalowanego(bez HOG): {numpy.amin(g_f_n_h[0])}\\n')\n",
    "\n",
    "print(f'Max arg z skalowanego(z HOG): {numpy.argmax(rescaled_features_gf[0])}, Min arg z skalowanego(z HOG): {numpy.argmin(rescaled_features_gf[0])}')\n",
    "print(f'Max z skalowanego(z HOG): {numpy.amax(rescaled_features_gf[0])}, Min z skalowanego(z HOG): {numpy.amin(rescaled_features_gf[0])}\\n')\n",
    "\n",
    "print(f'Max arg z skalowanego(bez HOG): {numpy.argmax(rescaled_features_gfnh[0])}, Min arg z skalowanego(bez HOG): {numpy.argmin(rescaled_features_gfnh[0])}')\n",
    "print(f'Max z skalowanego(bez HOG): {numpy.amax(rescaled_features_gfnh[0])}, Min z skalowanego(bez HOG): {numpy.amin(rescaled_features_gfnh[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zapis/Odczyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save(features, labels, dataframe, name):\n",
    "    h5f_data = h5py.File('data_' + str(name) + '.h5', 'w')\n",
    "    h5f_data.create_dataset('dataset', data=numpy.array(features))\n",
    "\n",
    "    h5f_label = h5py.File('labels_' + str(name) + '.h5', 'w')\n",
    "    h5f_label.create_dataset('dataset', data=numpy.array(labels))\n",
    "\n",
    "    h5f_data.close()\n",
    "    h5f_label.close()\n",
    "\n",
    "    dataframe.to_csv('dataframe_' + str(name) + '.csv')\n",
    "    \n",
    "def load(features, labels, dataframe):\n",
    "    h5f_data  = h5py.File(features, 'r')\n",
    "    h5f_label = h5py.File(labels, 'r')\n",
    "\n",
    "    global_features_string = h5f_data['dataset']\n",
    "    global_labels_string   = h5f_label['dataset']\n",
    "\n",
    "    global_features = numpy.array(global_features_string)\n",
    "    global_labels   = numpy.array(global_labels_string)\n",
    "\n",
    "    h5f_data.close()\n",
    "    h5f_label.close()\n",
    "    \n",
    "    df = pd.read_csv(dataframe, index_col = 0)  \n",
    "    \n",
    "    return global_features, global_labels, df\n",
    "    \n",
    "save(rescaled_features_gf, target, df, name='Casia_with_HOG')\n",
    "save(rescaled_features_gfnh, target, df, name='Casia_without_HOG')\n",
    "\n",
    "# B od BIG - więcej danych(z HOGiem), S - small(bez HOGa)\n",
    "global_features_B, global_labels_B, df_B = load('data_Casia_with_HOG.h5', 'labels_Casia_with_HOG.h5', 'dataframe_Casia_with_HOG.csv')\n",
    "global_features_S, global_labels_S, df_S = load('data_Casia_without_HOG.h5', 'labels_Casia_without_HOG.h5', 'dataframe_Casia_without_HOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "global_features_B_PCA = pca.fit_transform(global_features_B)\n",
    "global_features_S_PCA = pca.fit_transform(global_features_S)\n",
    "\n",
    "save(global_features_B_PCA, target, df, name='Casia_with_HOG_with_PCA')\n",
    "save(global_features_S_PCA, target, df, name='Casia_without_HOG_without_PCA')\n",
    "\n",
    "# B od BIG - więcej danych(z HOGiem), S - small(bez HOGa)\n",
    "global_features_B_PCA, global_labels_B_PCA, df_B_PCA = load('data_Casia_with_HOG_with_PCA.h5', 'labels_Casia_with_HOG_with_PCA.h5', 'dataframe_Casia_with_HOG_with_PCA.csv')\n",
    "global_features_S_PCA, global_labels_S_PCA, df_S_PCA = load('data_Casia_without_HOG_without_PCA.h5', 'labels_Casia_without_HOG_without_PCA.h5', 'dataframe_Casia_without_HOG_without_PCA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Z HOG:')\n",
    "print(f'\\t Wektor zdjęć: {global_features_B.shape}')\n",
    "print(f'\\t Wektor kategorii słownych: {global_labels_B.shape}\\n')\n",
    "print(f'\\t Wektor dataframe: {df_B.shape}\\n\\n')\n",
    "\n",
    "\n",
    "print(f'BEZ HOG:')\n",
    "print(f'\\t Wektor zdjęć: {global_features_S.shape}')\n",
    "print(f'\\t Wektor kategorii słownych: {global_labels_S.shape}\\n')\n",
    "print(f'\\t Wektor dataframe: {df_S.shape}\\n')\n",
    "\n",
    "\n",
    "print(f'Z HOG + PCA:')\n",
    "print(f'\\t Wektor zdjęć: {global_features_B_PCA.shape}')\n",
    "print(f'\\t Wektor kategorii słownych: {global_labels_B_PCA.shape}\\n')\n",
    "print(f'\\t Wektor dataframe: {df_B_PCA.shape}\\n\\n')\n",
    "\n",
    "\n",
    "print(f'BEZ HOG + PCA:')\n",
    "print(f'\\t Wektor zdjęć: {global_features_S_PCA.shape}')\n",
    "print(f'\\t Wektor kategorii słownych: {global_labels_S_PCA.shape}\\n')\n",
    "print(f'\\t Wektor dataframe: {df_S_PCA.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_S.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcję liczące statystyki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "def countStats(_y_true, _y_pred):\n",
    "    accuracy = accuracy_score(_y_true, _y_pred, normalize=True)\n",
    "    precision = precision_score(_y_true, _y_pred, average='weighted')\n",
    "    recall = recall_score(_y_true, _y_pred, average='weighted')\n",
    "    fscore = f1_score(_y_true, _y_pred, average='weighted')\n",
    "    \n",
    "    return accuracy, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "def plot_cm(cm, classes):\n",
    "    plot_confusion_matrix(conf_mat=cm,\n",
    "                          colorbar=True, \n",
    "                          show_absolute=True,\n",
    "                          show_normed=True,\n",
    "                          class_names=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcje do liczenia modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import clone\n",
    "import copy\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def do_the_thing(features, labels):\n",
    "    \n",
    "    clf_list = {\n",
    "        \"SVM linear\": [],\n",
    "        \"SVM poly\": [],\n",
    "        \"SVM rbf\": [],\n",
    "        \"SVM sigmoid\": []\n",
    "    }\n",
    "    \n",
    "    tcm_list = copy.deepcopy(clf_list)\n",
    "    tAccuracy_list = copy.deepcopy(clf_list)\n",
    "    tPrecision_list = copy.deepcopy(clf_list)\n",
    "    tRecall_list = copy.deepcopy(clf_list)\n",
    "    tFScore_list = copy.deepcopy(clf_list)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=odp)\n",
    "    clfs = {\n",
    "        \"SVM linear\": SVC(kernel='linear', probability=True, random_state=odp, verbose=True),\n",
    "        \"SVM poly\": SVC(kernel='poly', probability=True, random_state=odp, verbose=True),\n",
    "        \"SVM rbf\": SVC(kernel='rbf', probability=True, random_state=odp, verbose=True),\n",
    "        \"SVM sigmoid\": SVC(kernel='sigmoid', probability=True, random_state=odp, verbose=True),\n",
    "    }\n",
    "    scores = numpy.zeros((len(clfs), 5))\n",
    "    \n",
    "    for fold_id, (train_index, test_index) in enumerate(kf.split(features, labels)):\n",
    "        for clf_idx, clf_name in enumerate(clfs):\n",
    "            clf = clone(clfs[clf_name])\n",
    "            clf.fit(features[train_index], labels[train_index])\n",
    "            y_pred = clf.predict(features[test_index])\n",
    "\n",
    "            accuracy, precision, recall, fscore = countStats(labels[test_index], y_pred)\n",
    "            cm = confusion_matrix(labels[test_index], y_pred)\n",
    "            scores[clf_idx, fold_id] = accuracy_score(labels[test_index], y_pred)\n",
    "\n",
    "            tAccuracy_list[clf_name].append(accuracy)\n",
    "            tPrecision_list[clf_name].append(precision)\n",
    "            tRecall_list[clf_name].append(recall)\n",
    "            tFScore_list[clf_name].append(fscore)\n",
    "            tcm_list[clf_name].append(cm)\n",
    "\n",
    "    \n",
    "    print(f'\\n\\nKształt danych:')\n",
    "    print(f'\\t X_train: {features[train_index].shape}')\n",
    "    print(f'\\t X_test: {features[test_index].shape}')\n",
    "    print(f'\\t y_train: {labels[train_index].shape}')\n",
    "    print(f'\\t y_test: {labels[test_index].shape}')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for elem in clf_list:\n",
    "        accuracy_m = numpy.mean(tAccuracy_list[elem])\n",
    "        precision_m = numpy.mean(tPrecision_list[elem])\n",
    "        recall_m = numpy.mean(tRecall_list[elem])\n",
    "        fscore_m = numpy.mean(tFScore_list[elem])\n",
    "        \n",
    "        accuracy_std = numpy.std(tAccuracy_list[elem])\n",
    "        precision_std = numpy.std(tPrecision_list[elem])\n",
    "        recall_std = numpy.std(tRecall_list[elem])\n",
    "        fscore_std = numpy.std(tFScore_list[elem])\n",
    "        \n",
    "        cm = sum(tcm_list[elem])\n",
    "        \n",
    "        res = [str(elem), \n",
    "               f'{accuracy_m:.3f} ({accuracy_std:.2f})',\n",
    "               f'{precision_m:.3f} ({precision_std:.2f})',\n",
    "               f'{recall_m:.3f} ({recall_std:.2f})',\n",
    "               f'{fscore_m:.3f} ({fscore_std:.2f})',\n",
    "               f'{cm}']\n",
    "        results.append(res)\n",
    "\n",
    "    printmd(f'### Rezultaty:')\n",
    "        \n",
    "    headers = [\"Kernel\", \"Accuracy\", \"Precision\", \"Recall\", \"Fscore\", \"CM\"]\n",
    "    print('\\n')\n",
    "    print(tabulate(results, headers=headers))\n",
    "    \n",
    "    \n",
    "    printmd(f'### Analiza statystyczna:')\n",
    "    \n",
    "    alfa = .05\n",
    "    t_statistic = numpy.zeros((len(clfs), len(clfs)))\n",
    "    p_value = numpy.zeros((len(clfs), len(clfs)))\n",
    "\n",
    "    for i in range(len(clfs)):\n",
    "        for j in range(len(clfs)):\n",
    "            t_statistic[i, j], p_value[i, j] = ttest_ind(scores[i], scores[j])\n",
    "    headers = [\"SVM linear\", \"SVM poly\", \"SVM rbf\", \"SVM sigmoid\"]\n",
    "    names_column = numpy.array([[\"SVM linear\"], [\"SVM poly\"], [\"SVM rbf\"], [\"SVM sigmoid\"]])\n",
    "    t_statistic_table = numpy.concatenate((names_column, t_statistic), axis=1)\n",
    "    t_statistic_table = tabulate(t_statistic_table, headers, floatfmt=\".2f\")\n",
    "    p_value_table = numpy.concatenate((names_column, p_value), axis=1)\n",
    "    p_value_table = tabulate(p_value_table, headers, floatfmt=\".2f\")\n",
    "    printmd(\"**t-statistic:**\")\n",
    "    print(t_statistic_table)\n",
    "    printmd(\"**p-value:**\")\n",
    "    print(p_value_table)\n",
    "    \n",
    "    \n",
    "    advantage = numpy.zeros((len(clfs), len(clfs)))\n",
    "    advantage[t_statistic > 0] = 1\n",
    "    advantage_table = tabulate(numpy.concatenate(\n",
    "        (names_column, advantage), axis=1), headers)\n",
    "    printmd(\"**Przewaga:**\")\n",
    "    print(advantage_table)\n",
    "    \n",
    "    \n",
    "    significance = numpy.zeros((len(clfs), len(clfs)))\n",
    "    significance[p_value <= alfa] = 1\n",
    "    significance_table = tabulate(numpy.concatenate(\n",
    "        (names_column, significance), axis=1), headers)\n",
    "    printmd(\"**Różnice statystycznie znaczące (alpha = 0.05):**\")\n",
    "    print(significance_table)\n",
    "    \n",
    "    \n",
    "    stat_better = significance * advantage\n",
    "    stat_better_table = tabulate(numpy.concatenate(\n",
    "        (names_column, stat_better), axis=1), headers)\n",
    "    printmd(\"**Wynik końcowy analizy statystycznej:**\")\n",
    "    print(stat_better_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_the_thing(global_features_B, global_labels_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bez HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_the_thing(global_features_S, global_labels_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG -> PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_the_thing(global_features_B_PCA, global_labels_B_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bez HOG -> PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_the_thing(global_features_S_PCA, global_labels_S_PCA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
