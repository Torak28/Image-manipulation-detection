{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprawdzanie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie odpowiednich danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dla PoC wykonuje obliczenia dla:\n",
    " * '../data/DogsCats'\n",
    "Folder docelowy:\n",
    " * '../data/Photos'\n",
    "'''\n",
    "\n",
    "dir_path = '../../data/Photos'\n",
    "A_folder = 'originals'\n",
    "B_folder = 'photoshops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from keras.preprocessing.image import load_img\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# fix random bo tak ( ͡° ͜ʖ ͡°)\n",
    "odp = 42\n",
    "numpy.random.seed(odp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stałe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilkości\n",
    "\n",
    "IMAGE_WIDTH=128\n",
    "IMAGE_HEIGHT=128\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przygotowanie Danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Opis danych:\n",
    "1 - klasa 1 -> Originals\n",
    "0 - klasa 2 -> Photoshops\n",
    "''' \n",
    "\n",
    "A_folder_list = os.listdir(dir_path + '/' + A_folder)\n",
    "B_folder_list = os.listdir(dir_path + '/' + B_folder)\n",
    "\n",
    "filenames = []\n",
    "categories = []\n",
    "\n",
    "for filename in A_folder_list:\n",
    "    categories.append(0)\n",
    "    filenames.append(dir_path + '/' + A_folder + '/' + filename)\n",
    "\n",
    "for filename in B_folder_list:\n",
    "    categories.append(1)\n",
    "    filenames.append(dir_path + '/' + B_folder + '/' + filename)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mieszamy!\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(df['filename'])\n",
    "image = load_img(sample)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obróbka zdjęć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "import mahotas\n",
    "import cv2\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "\n",
    "def ft_histogram(image, mask=None):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # chanels: [0,1,2], bo mamy przestrzeń HSV\n",
    "    # mask: None\n",
    "    # histSize: [8, 8, 8], bin count, po 8 dla każdego z kanałów\n",
    "    # ranges : [0, 256, 0, 256, 0, 256], wszystko dla każdego z 3 kanałów\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def ft_hog(image):\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hog_features, hog_image = hog(image, block_norm='L2-Hys', pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "    return hog_features, hog_image\n",
    "\n",
    "\n",
    "def ft_ela(image):\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    im = Image.fromarray(image)\n",
    "    im.save('tmp.jpg', 'JPEG', quality=90)\n",
    "    resaved_im = Image.open('tmp.jpg')\n",
    "    \n",
    "    ela_im = ImageChops.difference(im, resaved_im)\n",
    "    \n",
    "    extrema = ela_im.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    if max_diff == 0:\n",
    "        max_diff = 1\n",
    "    scale = 255.0 / max_diff\n",
    "    \n",
    "    ela_im = ImageEnhance.Brightness(ela_im).enhance(scale)\n",
    "\n",
    "    return numpy.asarray(ela_im).flatten() / 255, ela_im\n",
    "    \n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, IMAGE_SIZE)\n",
    "        histogram = ft_histogram(image)\n",
    "        ela = ft_ela(image)[0]\n",
    "        hog = ft_hog(image)[0]\n",
    "        global_feature = numpy.hstack([histogram, ela, hog])\n",
    "        return global_feature\n",
    "    except Exception as e:\n",
    "        print(f'Problem with {image_path}, error_msg: {str(e)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(df['filename'])\n",
    "image = cv2.imread(sample)\n",
    "image = cv2.resize(image, IMAGE_SIZE)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ela = ft_ela(image)\n",
    "\n",
    "plt.imshow(ela[1])\n",
    "print(f'Kształt: {ela[0].shape}')\n",
    "print(f'Min: {numpy.amax(ela[0])}')\n",
    "print(f'Max: {numpy.amin(ela[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_img = ft_hog(image)\n",
    "\n",
    "plt.imshow(hog_img[1])\n",
    "print(f'Kształt: {hog_img[0].shape}')\n",
    "print(f'Min: {numpy.amax(hog_img[0])}')\n",
    "print(f'Max: {numpy.amin(hog_img[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = ft_histogram(image)\n",
    "\n",
    "print(f'Kształt: {histogram.shape}')\n",
    "print(f'Min: {numpy.amax(histogram)}')\n",
    "print(f'Max: {numpy.amin(histogram)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Wielkość całego wektora dla pojedynczego zdjęcia: {histogram.shape[0] + ela[0].shape[0] + hog_img[0].shape[0]}')\n",
    "print(f'\\t Histogram({histogram.shape[0]}): {(histogram.shape[0] * 100)/(histogram.shape[0] + ela[0].shape[0] + hog_img[0].shape[0]):.2f}%')\n",
    "print(f'\\t Ela({ela[0].shape[0]}): {(ela[0].shape[0] * 100)/(histogram.shape[0] + ela[0].shape[0] + hog_img[0].shape[0]):.2f}%')\n",
    "print(f'\\t Hog({hog_img[0].shape[0]}): {(hog_img[0].shape[0] * 100)/(histogram.shape[0] + ela[0].shape[0] + hog_img[0].shape[0]):.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przeliczenie Cech Zdjęć + Kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_f = []\n",
    "l = []\n",
    "\n",
    "for i in df['filename']:\n",
    "    data_img = preprocess_image(i)\n",
    "    g_f.append(data_img)\n",
    "\n",
    "for i in df['category']:\n",
    "    if i == 0:\n",
    "        l.append('original')\n",
    "    else:\n",
    "        l.append('photoshop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(f'Wektor zdjęć: {numpy.array(g_f).shape}')\n",
    "print(f'Wektor kategorii słownych: {numpy.array(l).shape}\\n')\n",
    "\n",
    "targetNames = numpy.unique(l)\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(l)\n",
    "\n",
    "\n",
    "print(f'Wektor kategorii liczbowych: {target.shape}')\n",
    "print(f'Max arg z nieskalowanego: {numpy.argmax(g_f[0])}, Min arg z nieskalowanego: {numpy.argmin(g_f[0])}')\n",
    "print(f'Max z nieskalowanego: {numpy.amax(g_f[0])}, Min z nieskalowanego: {numpy.amin(g_f[0])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zapis/Odczyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save(features, labels, dataframe, name):\n",
    "    h5f_data = h5py.File('data_' + str(name) + '.h5', 'w')\n",
    "    h5f_data.create_dataset('dataset', data=numpy.array(features))\n",
    "\n",
    "    h5f_label = h5py.File('labels_' + str(name) + '.h5', 'w')\n",
    "    h5f_label.create_dataset('dataset', data=numpy.array(labels))\n",
    "\n",
    "    h5f_data.close()\n",
    "    h5f_label.close()\n",
    "\n",
    "    dataframe.to_csv('dataframe_' + str(name) + '.csv')\n",
    "    \n",
    "def load(features, labels, dataframe):\n",
    "    h5f_data  = h5py.File(features, 'r')\n",
    "    h5f_label = h5py.File(labels, 'r')\n",
    "\n",
    "    global_features_string = h5f_data['dataset']\n",
    "    global_labels_string   = h5f_label['dataset']\n",
    "\n",
    "    global_features = numpy.array(global_features_string)\n",
    "    global_labels   = numpy.array(global_labels_string)\n",
    "\n",
    "    h5f_data.close()\n",
    "    h5f_label.close()\n",
    "    \n",
    "    df = pd.read_csv(dataframe, index_col = 0)  \n",
    "    \n",
    "    return global_features, global_labels, df\n",
    "    \n",
    "save(g_f, target, df, name='Casia')\n",
    "\n",
    "global_features, global_labels, df = load('data_Casia.h5', 'labels_Casia.h5', 'dataframe_Casia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Kształt po wczytaniu:')\n",
    "print(f'\\tglobal_features: {global_features.shape}')\n",
    "print(f'\\tglobal_labels: {global_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ela = global_features[0][512:3584] * 255\n",
    "ela = ela.astype(numpy.uint8)\n",
    "ela = ela.reshape(32, 32, 3)\n",
    "ela.shape\n",
    "\n",
    "print(f'Kształt: {ela.shape}')\n",
    "print(f'Min: {numpy.amax(ela)}')\n",
    "print(f'Max: {numpy.amin(ela)}')\n",
    "\n",
    "plt.imshow(Image.fromarray(ela))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_Casia = 3000\n",
    "n_sCasia = 100\n",
    "\n",
    "if global_features.shape[0] < 1000:\n",
    "    n_comps = n_sCasia\n",
    "else:\n",
    "    n_comps = n_Casia\n",
    "\n",
    "pca = PCA(n_components=n_comps)\n",
    "global_features_PCA = pca.fit_transform(global_features)\n",
    "\n",
    "save(global_features_PCA, target, df, name='Casia_with_PCA')\n",
    "\n",
    "global_features_PCA, global_labels_PCA, df_PCA = load('data_Casia_with_PCA.h5', 'labels_Casia_with_PCA.h5', 'dataframe_Casia_with_PCA.csv')\n",
    "del global_features, global_labels, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\t Wektor zdjęć: {global_features_PCA.shape}')\n",
    "print(f'\\t Wektor kategorii słownych: {global_labels_PCA.shape}\\n')\n",
    "print(f'\\t Wektor dataframe: {df_PCA.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcję liczące statystyki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "def countStats(_y_true, _y_pred):\n",
    "    accuracy = accuracy_score(_y_true, _y_pred, normalize=True)\n",
    "    precision = precision_score(_y_true, _y_pred, average='binary')\n",
    "    recall = recall_score(_y_true, _y_pred, average='binary')\n",
    "    fscore = f1_score(_y_true, _y_pred, average='binary')\n",
    "    \n",
    "    return accuracy, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "def plot_cm(cm, classes):\n",
    "    plot_confusion_matrix(conf_mat=cm,\n",
    "                          colorbar=True, \n",
    "                          show_absolute=True,\n",
    "                          show_normed=True,\n",
    "                          class_names=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis(scores, name, clfs, info=False):\n",
    "    printmd(f'### Analiza statystyczna dla {name}:')\n",
    "    \n",
    "    alfa = .05\n",
    "    t_statistic = numpy.zeros((len(clfs), len(clfs)))\n",
    "    p_value = numpy.zeros((len(clfs), len(clfs)))\n",
    "    \n",
    "    for i in range(len(clfs)):\n",
    "        for j in range(len(clfs)):\n",
    "            t_statistic[i, j], p_value[i, j] = ttest_ind(scores[i], scores[j])\n",
    "    headers = [\"SVM linear\", \"SVM poly\", \"SVM rbf\", \"SVM sigmoid\"]\n",
    "    names_column = numpy.array([[\"SVM linear\"], [\"SVM poly\"], [\"SVM rbf\"], [\"SVM sigmoid\"]])\n",
    "    t_statistic_table = numpy.concatenate((names_column, t_statistic), axis=1)\n",
    "    t_statistic_table = tabulate(t_statistic_table, headers, floatfmt=\".2f\")\n",
    "    p_value_table = numpy.concatenate((names_column, p_value), axis=1)\n",
    "    p_value_table = tabulate(p_value_table, headers, floatfmt=\".2f\")\n",
    "    \n",
    "    if info:\n",
    "        printmd(\"**t-statistic:**\")\n",
    "        print(t_statistic_table)\n",
    "        printmd(\"**p-value:**\")\n",
    "        print(p_value_table)\n",
    "    \n",
    "    \n",
    "    advantage = numpy.zeros((len(clfs), len(clfs)))\n",
    "    advantage[t_statistic > 0] = 1\n",
    "    advantage_table = tabulate(numpy.concatenate(\n",
    "        (names_column, advantage), axis=1), headers)\n",
    "\n",
    "    if info:\n",
    "        printmd(\"**Przewaga:**\")\n",
    "        print(advantage_table)\n",
    "    \n",
    "    \n",
    "    significance = numpy.zeros((len(clfs), len(clfs)))\n",
    "    significance[p_value <= alfa] = 1\n",
    "    significance_table = tabulate(numpy.concatenate(\n",
    "        (names_column, significance), axis=1), headers)\n",
    "    \n",
    "    if info:\n",
    "        printmd(\"**Różnice statystycznie znaczące (alpha = 0.05):**\")\n",
    "        print(significance_table)\n",
    "    \n",
    "    \n",
    "    stat_better = significance * advantage\n",
    "    stat_better_table = tabulate(numpy.concatenate(\n",
    "        (names_column, stat_better), axis=1), headers)\n",
    "    printmd(\"**Wynik końcowy analizy statystycznej:**\")\n",
    "    print(stat_better_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcje do liczenia modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import clone\n",
    "import copy\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def do_the_thing(features, labels, name):\n",
    "    \n",
    "    clf_list = {\n",
    "        \"SVM linear\": [],\n",
    "        \"SVM poly\": [],\n",
    "        \"SVM rbf\": [],\n",
    "        \"SVM sigmoid\": []\n",
    "    }\n",
    "    \n",
    "    tcm_list = copy.deepcopy(clf_list)\n",
    "    tAccuracy_list = copy.deepcopy(clf_list)\n",
    "    tPrecision_list = copy.deepcopy(clf_list)\n",
    "    tRecall_list = copy.deepcopy(clf_list)\n",
    "    tFScore_list = copy.deepcopy(clf_list)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=odp)\n",
    "    clfs = {\n",
    "        \"SVM linear\": SVC(kernel='linear', probability=True, random_state=odp, verbose=True, cache_size=1000),\n",
    "        \"SVM poly\": SVC(kernel='poly', probability=True, random_state=odp, verbose=True, cache_size=1000),\n",
    "        \"SVM rbf\": SVC(kernel='rbf', probability=True, random_state=odp, verbose=True, cache_size=1000),\n",
    "        \"SVM sigmoid\": SVC(kernel='sigmoid', probability=True, random_state=odp, verbose=True, cache_size=1000),\n",
    "    }\n",
    "    scores_a = numpy.zeros((len(clfs), 5))\n",
    "    scores_p = numpy.zeros((len(clfs), 5))\n",
    "    scores_r = numpy.zeros((len(clfs), 5))\n",
    "    scores_f = numpy.zeros((len(clfs), 5))\n",
    "    \n",
    "    for fold_id, (train_index, test_index) in enumerate(kf.split(features, labels)):\n",
    "        for clf_idx, clf_name in enumerate(clfs):\n",
    "            clf = clone(clfs[clf_name])\n",
    "            clf.fit(features[train_index], labels[train_index])\n",
    "            y_pred = clf.predict(features[test_index])\n",
    "\n",
    "            accuracy, precision, recall, fscore = countStats(labels[test_index], y_pred)\n",
    "            cm = confusion_matrix(labels[test_index], y_pred)\n",
    "            scores_a[clf_idx, fold_id] = accuracy_score(labels[test_index], y_pred, normalize=True)\n",
    "            scores_p[clf_idx, fold_id] = precision_score(labels[test_index], y_pred, average='binary')\n",
    "            scores_r[clf_idx, fold_id] = recall_score(labels[test_index], y_pred, average='binary')\n",
    "            scores_f[clf_idx, fold_id] = f1_score(labels[test_index], y_pred, average='binary')\n",
    "\n",
    "            tAccuracy_list[clf_name].append(accuracy)\n",
    "            tPrecision_list[clf_name].append(precision)\n",
    "            tRecall_list[clf_name].append(recall)\n",
    "            tFScore_list[clf_name].append(fscore)\n",
    "            tcm_list[clf_name].append(cm)\n",
    "\n",
    "    printmd(f'# {name}:')\n",
    "    print(f'\\n\\nKształt danych:')\n",
    "    print(f'\\t X_train: {features[train_index].shape}')\n",
    "    print(f'\\t X_test: {features[test_index].shape}')\n",
    "    print(f'\\t y_train: {labels[train_index].shape}')\n",
    "    print(f'\\t y_test: {labels[test_index].shape}')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for elem in clf_list:\n",
    "        accuracy_m = numpy.mean(tAccuracy_list[elem])\n",
    "        precision_m = numpy.mean(tPrecision_list[elem])\n",
    "        recall_m = numpy.mean(tRecall_list[elem])\n",
    "        fscore_m = numpy.mean(tFScore_list[elem])\n",
    "        \n",
    "        accuracy_std = numpy.std(tAccuracy_list[elem])\n",
    "        precision_std = numpy.std(tPrecision_list[elem])\n",
    "        recall_std = numpy.std(tRecall_list[elem])\n",
    "        fscore_std = numpy.std(tFScore_list[elem])\n",
    "        \n",
    "        cm = sum(tcm_list[elem])\n",
    "        \n",
    "        res = [str(elem), \n",
    "               f'{accuracy_m:.3f} ({accuracy_std:.2f})',\n",
    "               f'{precision_m:.3f} ({precision_std:.2f})',\n",
    "               f'{recall_m:.3f} ({recall_std:.2f})',\n",
    "               f'{fscore_m:.3f} ({fscore_std:.2f})',\n",
    "               f'{cm}']\n",
    "        results.append(res)\n",
    "\n",
    "    printmd(f'### Rezultaty:')\n",
    "        \n",
    "    headers = [\"Kernel\", \"Accuracy\", \"Precision\", \"Recall\", \"Fscore\", \"CM\"]\n",
    "    print('\\n')\n",
    "    print(tabulate(results, headers=headers))\n",
    "    \n",
    "    with open(f'{name}_result.txt', 'w') as f:\n",
    "        print(tabulate(results, headers=headers), file=f)\n",
    "    \n",
    "    numpy.save(f'{name}_results_a', scores_a)\n",
    "    numpy.save(f'{name}_results_p', scores_p)\n",
    "    numpy.save(f'{name}_results_r', scores_r)\n",
    "    numpy.save(f'{name}_results_f', scores_f)\n",
    "    \n",
    "    statistical_analysis(scores_a, name=\"Accuracy\", clfs=[\"SVM linear\", \"SVM poly\", \"SVM rbf\", \"SVM sigmoid\"])\n",
    "    statistical_analysis(scores_p, name=\"Precision\", clfs=[\"SVM linear\", \"SVM poly\", \"SVM rbf\", \"SVM sigmoid\"])\n",
    "    statistical_analysis(scores_r, name=\"Recall\", clfs=[\"SVM linear\", \"SVM poly\", \"SVM rbf\", \"SVM sigmoid\"])\n",
    "    statistical_analysis(scores_f, name=\"Fscore\", clfs=[\"SVM linear\", \"SVM poly\", \"SVM rbf\", \"SVM sigmoid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wynik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_the_thing(global_features_PCA, global_labels_PCA, name='PCA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
